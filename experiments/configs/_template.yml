# Reusable Experiment Template
## How to use
# 1) Copy this file and rename it, or edit in-place (change `name:`).
# 2) Define one or more targets via `targets:`.
# 3) Adjust input/output paths, prompt template, parser, and concurrency.
#
## Sections overview
# - Name
# - Target selection
# - Input data
# - Input filters
# - Output data
# - Output configuration
# - Prompt
# - Parsing
# - Concurrency
# - Execution
# - CLI tips

## Name
# Recommended naming scheme
# Use: exp<N>_<domain>_<task>[_<variant>]
# - <domain>: cnf | horn
# - <task>: contradiction | yesno | cot
# - <variant> (optional): linear | parents | v1 | v2
# Examples
# - exp1_cnf_v1
# - exp3_cnf_cot_linear
# - exp4_cnf_cot_parents
# - exp5_horn_cot_linear
# - exp6_horn_yesno
name: CHANGE_ME_experiment_name

## Target selection
# Thinking tiers for comparison:
# - Flagship: highest thinking effort/budget
# - Middle: moderate thinking effort/budget
# - Budget/Fast: thinking disabled
# Models for each thinking tier:
# - High effort (Flagship): claude-opus-4-1-20250805, gemini-2.5-pro, gpt-5-2025-08-07
# - Middle effort (Middle): claude-sonnet-4-20250514, gemini-2.5-flash, gpt-5-mini-2025-08-07
# - Low/Budget (Fast): claude-3-5-haiku-20241022, gemini-2.5-flash-lite, gpt-5-nano-2025-08-07
# Tokens for each thinking tier:
# - This is informed by the mapping of low/medium/high reasoning effort to tokens in the compatibility documentation: https://ai.google.dev/gemini-api/docs/openai#thinking
# - High effort (Flagship): 24,576
# - Middle effort (Middle): 8,192
# - Low/Budget (Fast): 
# Official documentation on thinking:
# - Anthropic: https://docs.claude.com/en/docs/build-with-claude/extended-thinking
# - Google: https://ai.google.dev/gemini-api/docs/thinking
# - OpenAI: https://platform.openai.com/docs/guides/reasoning/advice-on-prompting#get-started-with-reasoning
# Notes:
# - Anthropic requires temperature=1 when thinking is enabled
# - For Anthropic, max_tokens must be > thinking.budget_tokens
# - For Google, thinking uses thinkingConfig.thinkingBudget
# - For OpenAI, reasoning effort can be high/medium; disabled for budget tier
# - For a single-target run, keep ONE item in this list
# - To run multiple models for the same provider, add another target entry
targets:
  - provider: anthropic
    model: claude-opus-4-1-20250805    # Model identifier for the provider
    temperature: 0                     # Sampling temperature (0 for deterministic)
    seed: 1234                         # Optional seed for reproducibility
    max_tokens: 2000                   # Max output tokens
    thinking:                          # Optional extended thinking (Anthropic: summarized thinking)
      enabled: true
      budget_tokens: 8192
  - provider: anthropic
    model: claude-sonnet-4-20250514
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:
      enabled: true
      budget_tokens: 4096
  - provider: anthropic
    model: claude-3-5-haiku-20241022
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:
      enabled: false
  - provider: google
    model: gemini-2.5-pro
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:                          # Gemini: thinkingConfig.thinkingBudget
      enabled: true
      gemini_budget_tokens: 4096
  - provider: google
    model: gemini-2.5-flash
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:
      enabled: false                   # We default to disabling thinking on Flash if not provided
  - provider: google
    model: gemini-2.5-flash-lite
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:
      enabled: false
  - provider: openai
    model: gpt-5-2025-08-07
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:                          # OpenAI: Responses API reasoning
      enabled: true
      openai_reasoning:
        effort: medium
  - provider: openai
    model: gpt-5-mini-2025-08-07
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:
      enabled: true
      openai_reasoning:
        effort: low
  - provider: openai
    model: gpt-5-nano-2025-08-07
    temperature: 0
    seed: 1234
    max_tokens: 2000
    thinking:
      enabled: false

## Input data
input_file: data/problems_dist20_v1.js # Path to dataset (JSONL-like JS array lines)

## Input filters
filters:
  horn_only: true                      # Keep only horn problems
  skip_rows: 1                         # Skip a header row if present
  limit_rows: null                     # For quick tests; null processes all

## Output data
output_pattern: experiments/runs/${name}/${run}/${provider}/${model}/results.jsonl

## Output configuration
outputs:
  results:
    enabled: true                      # When false, results.jsonl is not written
  provenance:
    enabled: true                      # When false, results.provenance.jsonl is not written
    include_prompt: true               # Include rendered prompt in provenance
    include_raw_response: true         # Include provider raw payload in provenance

## Prompt
prompt:
  template: prompts/exp6_horn_yesno.j2 # Jinja2 template path
  style: horn_if_then                  # One of: horn_if_then | cnf_v1 | cnf_v2
  variables: {}                        # Optional template vars

## Parsing
# Parser maps model text â†’ parsed_answer used in stats:
#   0 = YES/CONTRADICTION, 1 = NO/SATISFIABLE, 2 = UNCLEAR
# For yes_no, we scan the text case-insensitively; first matching token wins.
# If no token matches, parsed_answer=2 and it's counted as UNCLEAR.
# Results also include normalized_text derived from parsed_answer ("yes"/"no").
parse:
  type: yes_no                         # One of: yes_no | contradiction
  yes_tokens: ["yes"]                  # Extra tokens treated as yes (e.g., "yep", "affirmative")
  no_tokens: ["no"]                    # Extra tokens treated as no (e.g., "nope", "negative")

## Concurrency
# Throttling guidance:
# - Effective concurrency per problem = min(workers, number of targets)
# - Set workers >= targets to run all targets in parallel (fastest)
# - Set workers < targets to throttle (reduces 429s/overload spikes, plays nicer with provider limits)
# Examples:
#   workers: 99   # "fast" variant: all targets parallel for each problem
#   workers: 2    # "throttled" variant: at most two requests in flight per problem
concurrency:
  workers: 9                           # Per-problem fan-out workers
  lockstep: true                       # Evaluate the same problem across targets concurrently (lockstep)
  targets_workers: 2                   # Reserved: cap concurrent provider/model groups (not enforced yet)
  rate_limit_per_min: 120              # Reserved: intended per-target limit (not enforced yet)
  retry:
    max_attempts: 3                    # Retries for transient errors (429/overloaded)
    backoff_seconds: [2, 5, 10]        # Per-attempt wait times (seconds); last value reused on further retries

## Execution
resume: true                           # Resume from prior results when re-running

# Optional global thinking defaults (applied to targets without per-target thinking)
thinking:
  enabled: false
  budget_tokens: 2048                  # Anthropic default if applied
  gemini_budget_tokens: 2048           # Gemini default if applied
  openai_reasoning:
    effort: medium

## CLI tips
# - Limit rows quickly:            --limit 100
# - Dry-run without API calls:     --dry-run
# - Resume interrupted runs:       --resume
# - Restrict providers:            --only anthropic,openai
# - Override models per provider:  --models google:gemini-2.5-pro,openai:gpt-5,
# - Inject run id into ${run}:     --run demo-123
