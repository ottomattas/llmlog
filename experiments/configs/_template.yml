# Reusable experiment template
# How to use:
# 1) Copy this file and rename it, or edit in-place (change `name:`)
# 2) Define target or targets via `targets:[]`.
# 3) Adjust input/output paths, prompt template, parser, and concurrency

name: CHANGE_ME_experiment_name

# Naming scheme (recommended)
# Use: exp<N>_<domain>_<task>[_<variant>]
# - <domain>: cnf | horn
# - <task>: contradiction | yesno | cot
# - <variant> (optional): linear | parents | v1 | v2
# Examples:
# - exp1_cnf_v1
# - exp3_cnf_cot_linear
# - exp4_cnf_cot_parents
# - exp5_horn_cot_linear
# - exp6_horn_yesno

# Target selection
# For a single-target run, keep ONE item in this list.
targets:
  - provider: anthropic
    model: claude-opus-4-1            # Model identifier for the provider
    temperature: 0                    # Sampling temperature (0 for deterministic)
    seed: 1234                        # Optional seed for reproducibility
    max_tokens: 2000                  # Max output tokens
  - provider: google
    model: gemini-2.5-flash
    temperature: 0
    seed: 1234
    max_tokens: 2000
    # To run multiple models for the same provider, add another target entry
    # with the same provider and a different model, e.g.:
    # (uncomment to use)
    #
    # - provider: google
    #   model: gemini-2.5-pro
    #   temperature: 0
    #   seed: 1234
    #   max_tokens: 2000
  - provider: openai
    model: gpt-5
    temperature: 0
    seed: 1234
    max_tokens: 2000

# Input dataset and outputs
input_file: data/problems_dist20_v1.js       # Path to dataset (JSONL-like JS array lines)
# Choose one output style:
# - Nested pattern (recommended for multi-targets):
output_pattern: experiments/runs/${name}/${run}/${provider}/${model}/results.jsonl  # ${run} set via --run
# - Flat pattern (alternative):
# output_pattern: experiments/runs/${name}/${provider}_${model}.jsonl
# - Single file (for single-target runs):
# output_file: experiments/runs/${name}/results.jsonl

# Filters
filters:
  horn_only: true             # Keep only horn problems
  skip_rows: 1                # Skip a header row if present
  limit_rows: null            # For quick tests; null processes all

# Prompt configuration
prompt:
  template: prompts/exp8_if_then_yesno.j2    # Jinja2 template path
  style: horn_if_then                        # horn_if_then | cnf_v1 | cnf_v2
  variables: {}                              # Optional template vars

# Parser configuration
parse:
  # Parser maps model text â†’ parsed_answer used in stats:
  #   0 = YES/CONTRADICTION, 1 = NO/SATISFIABLE, 2 = UNCLEAR
  # For yes_no, we scan the text case-insensitively; first matching token wins.
  # If no token matches, parsed_answer=2 and it's counted as UNCLEAR.
  # Results also include normalized_text derived from parsed_answer ("yes"/"no").
  type: yes_no               # yes_no | contradiction
  yes_tokens: ["yes"]       # Extra tokens treated as yes (e.g., "yep", "affirmative")
  no_tokens: ["no"]        # Extra tokens treated as no (e.g., "nope", "negative")

# Concurrency and retry settings
concurrency:
  workers: 4                 # Per-problem fan-out workers
  lockstep: true             # Evaluate the same problem across targets concurrently (lockstep)
  # Throttling guidance:
  # - Effective concurrency per problem = min(workers, number of targets)
  # - Set workers >= targets to run all targets in parallel (fastest)
  # - Set workers < targets to throttle (reduces 429s/overload spikes, plays nicer with provider limits)
  # Examples:
  #   workers: 99   # "fast" variant: all targets parallel for each problem
  #   workers: 2    # "throttled" variant: at most two requests in flight per problem
  targets_workers: 2         # Reserved: cap concurrent provider/model groups (not enforced yet)
  rate_limit_per_min: 120    # Reserved: intended per-target limit (not enforced yet)
  retry:
    max_attempts: 3          # Retries for transient errors (429/overloaded)
    backoff_seconds: [2, 5, 10]

# Execution settings
resume: true                 # resume from prior results when re-running

# Unified outputs
outputs:
  results:
    enabled: true            # When false, results.jsonl is not written
  provenance:
    enabled: true            # When false, results.provenance.jsonl is not written
    include_prompt: true     # Include rendered prompt in provenance
    include_raw_response: true  # Include provider raw payload in provenance
