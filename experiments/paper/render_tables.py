#!/usr/bin/env python3
"""
Generate paper-friendly summary tables from aggregated metrics.

Outputs (under --out-dir):
  - tables/model_summary.csv
  - tables/per_model/<provider>__<model>__summary.csv
  - tables/per_model/<provider>__<model>__summary.tex
"""

from __future__ import annotations

import argparse
import csv
import json
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


@dataclass
class Counts:
    total: int = 0
    correct: int = 0
    unclear: int = 0
    sat_total: int = 0
    sat_correct: int = 0
    unsat_total: int = 0
    unsat_correct: int = 0


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def _safe_filename(s: str) -> str:
    keep = []
    for ch in s:
        if ch.isalnum() or ch in ("-", "_", "."):
            keep.append(ch)
        else:
            keep.append("_")
    return "".join(keep)


def _int(s: Optional[str]) -> int:
    try:
        if not s:
            return 0
        return int(float(s))
    except Exception:
        return 0


def _rate(n: int, d: int) -> Optional[float]:
    if d <= 0:
        return None
    return n / d


def _fmt(x: Optional[float]) -> str:
    if x is None:
        return ""
    return f"{x:.3f}"


def _load_metrics_by_bucket(path: Path) -> List[Dict[str, str]]:
    with path.open("r", encoding="utf-8", newline="") as f:
        return list(csv.DictReader(f))


def _load_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def _write_tex_table(rows: List[Dict[str, Any]], out_path: Path, *, model_label: str) -> None:
    """
    Minimal LaTeX table (tabular) suitable for pasting. No special packages assumed.
    """
    header = [
        "prompt_id",
        "prompt_style",
        "thinking",
        "horn",
        "n",
        "acc",
        "sat_acc",
        "unsat_acc",
        "unclear",
    ]
    lines: List[str] = []
    lines.append("% Auto-generated by experiments/paper/render_tables.py\n")
    lines.append(f"% Model: {model_label}\n")
    lines.append("\\begin{tabular}{lllc r r r r r}\n")
    lines.append("\\hline\n")
    lines.append(" & ".join(header) + " \\\\\n")
    lines.append("\\hline\n")
    for r in rows:
        line = [
            str(r.get("prompt_id", "")),
            str(r.get("prompt_style", "")),
            str(r.get("thinking_mode", "")) or "n/a",
            str(r.get("horn", "")),
            str(r.get("total", "")),
            _fmt(r.get("accuracy")),
            _fmt(r.get("sat_accuracy")),
            _fmt(r.get("unsat_accuracy")),
            str(r.get("unclear", "")),
        ]
        # Escape underscores minimally
        line = [s.replace("_", "\\_") if isinstance(s, str) else str(s) for s in line]
        lines.append(" & ".join(line) + " \\\\\n")
    lines.append("\\hline\n")
    lines.append("\\end{tabular}\n")
    out_path.write_text("".join(lines), encoding="utf-8")


def main() -> int:
    ap = argparse.ArgumentParser(description="Render summary tables for paper from metrics")
    ap.add_argument("--data-dir", default="experiments/paper_outputs/data", help="Data directory")
    ap.add_argument("--out-dir", default="experiments/paper_outputs", help="Output directory root")
    args = ap.parse_args()

    data_dir = Path(args.data_dir).resolve()
    out_dir = Path(args.out_dir).resolve()
    tables_dir = out_dir / "tables"
    per_model_dir = tables_dir / "per_model"
    _ensure_dir(tables_dir)
    _ensure_dir(per_model_dir)

    metrics_path = data_dir / "metrics_by_bucket.csv"
    prompt_index = _load_json(data_dir / "prompt_index.json")

    rows = _load_metrics_by_bucket(metrics_path)

    # Aggregate across (experiment, run_id, maxvars, maxlen) -> model/prompt/thinking/horn totals
    # Key: (provider, model, thinking_mode, prompt_id, horn)
    agg: Dict[Tuple[str, str, str, str, int], Counts] = defaultdict(Counts)
    prompt_meta: Dict[str, Dict[str, str]] = {}

    for r in rows:
        provider = (r.get("provider") or "").strip()
        model = (r.get("model") or "").strip()
        thinking = (r.get("thinking_mode") or "").strip()
        prompt_id = (r.get("prompt_id") or "").strip()
        horn = _int(r.get("horn"))

        key = (provider, model, thinking, prompt_id, horn)
        c = agg[key]
        c.total += _int(r.get("total"))
        c.correct += _int(r.get("correct"))
        c.unclear += _int(r.get("unclear"))
        c.sat_total += _int(r.get("sat_total"))
        c.sat_correct += _int(r.get("sat_correct"))
        c.unsat_total += _int(r.get("unsat_total"))
        c.unsat_correct += _int(r.get("unsat_correct"))

        prompt_meta.setdefault(
            prompt_id,
            {
                "prompt_template": (r.get("prompt_template") or "").strip(),
                "prompt_style": (r.get("prompt_style") or "").strip(),
                "parse_family": (r.get("parse_family") or "").strip(),
            },
        )

    out_rows: List[Dict[str, Any]] = []
    for (provider, model, thinking, prompt_id, horn), c in sorted(agg.items()):
        pm = prompt_meta.get(prompt_id, {})
        out_rows.append(
            {
                "provider": provider,
                "model": model,
                "thinking_mode": thinking,
                "prompt_id": prompt_id,
                "prompt_style": pm.get("prompt_style", ""),
                "parse_family": pm.get("parse_family", ""),
                "prompt_template": pm.get("prompt_template", ""),
                "horn": horn,
                "total": c.total,
                "correct": c.correct,
                "accuracy": _rate(c.correct, c.total),
                "unclear": c.unclear,
                "unclear_rate": _rate(c.unclear, c.total),
                "sat_total": c.sat_total,
                "sat_correct": c.sat_correct,
                "sat_accuracy": _rate(c.sat_correct, c.sat_total),
                "unsat_total": c.unsat_total,
                "unsat_correct": c.unsat_correct,
                "unsat_accuracy": _rate(c.unsat_correct, c.unsat_total),
            }
        )

    # Write global CSV
    global_csv = tables_dir / "model_summary.csv"
    fields = [
        "provider",
        "model",
        "thinking_mode",
        "prompt_id",
        "prompt_style",
        "parse_family",
        "prompt_template",
        "horn",
        "total",
        "correct",
        "accuracy",
        "unclear",
        "unclear_rate",
        "sat_total",
        "sat_correct",
        "sat_accuracy",
        "unsat_total",
        "unsat_correct",
        "unsat_accuracy",
    ]
    with global_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        for r in out_rows:
            rr = dict(r)
            # format floats for CSV readability
            for k in ("accuracy", "unclear_rate", "sat_accuracy", "unsat_accuracy"):
                rr[k] = _fmt(rr.get(k))
            w.writerow(rr)

    # Per-model CSV + LaTeX
    by_model: Dict[Tuple[str, str], List[Dict[str, Any]]] = defaultdict(list)
    for r in out_rows:
        by_model[(r["provider"], r["model"])].append(r)

    for (provider, model), mrows in sorted(by_model.items()):
        slug = _safe_filename(f"{provider}__{model}")
        per_csv = per_model_dir / f"{slug}__summary.csv"
        per_tex = per_model_dir / f"{slug}__summary.tex"

        # sort within model for readability
        mrows_sorted = sorted(mrows, key=lambda x: (x.get("prompt_style", ""), x.get("prompt_id", ""), x.get("thinking_mode", ""), x.get("horn", 0)))

        with per_csv.open("w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=fields)
            w.writeheader()
            for r in mrows_sorted:
                rr = dict(r)
                for k in ("accuracy", "unclear_rate", "sat_accuracy", "unsat_accuracy"):
                    rr[k] = _fmt(rr.get(k))
                w.writerow(rr)

        _write_tex_table(mrows_sorted, per_tex, model_label=f"{provider}/{model}")

    print(f"Wrote:\n- {global_csv}\n- {per_model_dir}/<provider>__<model>__summary.(csv|tex)")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


